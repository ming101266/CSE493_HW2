model:
  block_size: 5
  vocab_size: 16
  n_layer: 2
  n_head: 4
  n_embd: 128
  dropout: 0.1
  bias: false
  tokenizer: "char-level"

train:
  batch_size: 100
  save_dir: "part2_checkpoints/"
  lr: 5e-4
  weight_decay: 0.5
  betas: [0.9, 0.95]
  epochs: 200
  save_interval: 10
  eval_interval: 1
  seed: 42

data: 
  data_dir: "part2_checkpoints/"

generate:
  seed: 42
  op: '/'
  prime: 97
  num_samples: 10000
  data_dir: "part2_checkpoints/"