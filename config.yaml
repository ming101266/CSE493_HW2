model:
  block_size: 2
  vocab_size: 50304
  n_layer: 1
  n_head: 1
  n_embd: 128
  dropout: 0.1
  bias: false
  tokenizer: gpt2
train:
  batch_size: 2
  save_dir: "checkpoints/"
  data_path: "input.txt"
  lr: 3e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  epochs: 300
  mask_first_three: false
  seed: 1