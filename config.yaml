model:
  block_size: 6
  vocab_size: 50304
  n_layer: 1
  n_head: 1
  n_embd: 768
  dropout: 0.1
  bias: false
  tokenizer: gpt2
train:
  batch_size: 4
  save_dir: "checkpoints/"
  data_path: "input.txt"
  lr: 3e-4
  weight_decay: 0.96
  betas: [0.9, 0.95]
  epochs: 301
  mask_first_three: false